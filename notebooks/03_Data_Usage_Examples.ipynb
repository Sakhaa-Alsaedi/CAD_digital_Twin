{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Data Usage Examples - Cardiac Digital Twins Enhanced\\n",
    "\\n",
    "This notebook demonstrates how to load, explore, and use the synthetic datasets provided with the Cardiac Digital Twins Enhanced framework.\\n",
    "\\n",
    "## üéØ Learning Objectives\\n",
    "\\n",
    "By the end of this notebook, you will be able to:\\n",
    "- Load and explore the synthetic cardiac datasets\\n",
    "- Understand the relationship between parameters and clinical metrics\\n",
    "- Prepare data for machine learning models\\n",
    "- Visualize cardiac function across different conditions\\n",
    "- Use the data for physics-informed learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\\n",
    "import pandas as pd\\n",
    "import numpy as np\\n",
    "import matplotlib.pyplot as plt\\n",
    "import seaborn as sns\\n",
    "from pathlib import Path\\n",
    "import warnings\\n",
    "warnings.filterwarnings('ignore')\\n",
    "\\n",
    "# Set style for better plots\\n",
    "plt.style.use('seaborn-v0_8')\\n",
    "sns.set_palette('husl')\\n",
    "\\n",
    "# Configure display\\n",
    "pd.set_option('display.max_columns', None)\\n",
    "pd.set_option('display.precision', 3)\\n",
    "\\n",
    "print('üìä Data Usage Examples - Cardiac Digital Twins Enhanced')\\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ 1. Loading the Datasets\\n",
    "\\n",
    "Let's start by loading all the available datasets and exploring their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\\n",
    "data_dir = Path('../data')\\n",
    "\\n",
    "# Load all datasets\\n",
    "datasets = {}\\n",
    "\\n",
    "# Windkessel parameters\\n",
    "datasets['parameters'] = pd.read_csv(data_dir / 'windkessel_parameters.csv')\\n",
    "print(f'‚úÖ Loaded Windkessel parameters: {datasets[\\\"parameters\\\"].shape}')\\n",
    "\\n",
    "# Clinical metrics\\n",
    "datasets['metrics'] = pd.read_csv(data_dir / 'clinical_metrics.csv')\\n",
    "print(f'‚úÖ Loaded clinical metrics: {datasets[\\\"metrics\\\"].shape}')\\n",
    "\\n",
    "# Combined dataset\\n",
    "datasets['combined'] = pd.read_csv(data_dir / 'combined_dataset.csv')\\n",
    "print(f'‚úÖ Loaded combined dataset: {datasets[\\\"combined\\\"].shape}')\\n",
    "\\n",
    "# Clinical conditions\\n",
    "datasets['conditions'] = pd.read_csv(data_dir / 'clinical_conditions.csv')\\n",
    "print(f'‚úÖ Loaded clinical conditions: {datasets[\\\"conditions\\\"].shape}')\\n",
    "\\n",
    "# Echo metadata\\n",
    "datasets['echo_meta'] = pd.read_csv(data_dir / 'echo_metadata.csv')\\n",
    "print(f'‚úÖ Loaded echo metadata: {datasets[\\\"echo_meta\\\"].shape}')\\n",
    "\\n",
    "print(f'\\\\nüìä Total datasets loaded: {len(datasets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 2. Dataset Exploration\\n",
    "\\n",
    "Let's explore the structure and content of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Windkessel parameters\\n",
    "print('üî¨ WINDKESSEL PARAMETERS')\\n",
    "print('=' * 40)\\n",
    "print(f'Shape: {datasets[\\\"parameters\\\"].shape}')\\n",
    "print(f'Columns: {list(datasets[\\\"parameters\\\"].columns)}')\\n",
    "print('\\\\nFirst 5 rows:')\\n",
    "display(datasets['parameters'].head())\\n",
    "\\n",
    "print('\\\\nStatistical summary:')\\n",
    "display(datasets['parameters'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore clinical metrics\\n",
    "print('üè• CLINICAL METRICS')\\n",
    "print('=' * 40)\\n",
    "print(f'Shape: {datasets[\\\"metrics\\\"].shape}')\\n",
    "print(f'Columns: {list(datasets[\\\"metrics\\\"].columns)}')\\n",
    "print('\\\\nFirst 5 rows:')\\n",
    "display(datasets['metrics'].head())\\n",
    "\\n",
    "print('\\\\nStatistical summary:')\\n",
    "display(datasets['metrics'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore clinical conditions\\n",
    "print('ü©∫ CLINICAL CONDITIONS')\\n",
    "print('=' * 40)\\n",
    "print(f'Shape: {datasets[\\\"conditions\\\"].shape}')\\n",
    "print(f'Conditions: {datasets[\\\"conditions\\\"][\\\"condition\\\"].unique()}')\\n",
    "print('\\\\nSamples per condition:')\\n",
    "print(datasets['conditions']['condition'].value_counts())\\n",
    "\\n",
    "print('\\\\nFirst 5 rows:')\\n",
    "display(datasets['conditions'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 3. Data Visualization\\n",
    "\\n",
    "Let's create visualizations to understand the data distributions and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter distributions\\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\\n",
    "fig.suptitle('Windkessel Parameter Distributions', fontsize=16, fontweight='bold')\\n",
    "\\n",
    "params = ['Emax', 'Emin', 'Tc', 'Rm', 'Ra', 'Rs', 'Ca', 'Cs', 'Cr', 'Ls', 'Rc', 'Vd']\\n",
    "\\n",
    "for i, param in enumerate(params):\\n",
    "    ax = axes[i//4, i%4]\\n",
    "    datasets['parameters'][param].hist(bins=30, ax=ax, alpha=0.7, color='skyblue', edgecolor='black')\\n",
    "    ax.set_title(f'{param}', fontweight='bold')\\n",
    "    ax.set_xlabel(param)\\n",
    "    ax.set_ylabel('Frequency')\\n",
    "    ax.grid(True, alpha=0.3)\\n",
    "\\n",
    "plt.tight_layout()\\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clinical metrics distributions\\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\\n",
    "fig.suptitle('Clinical Metrics Distributions', fontsize=16, fontweight='bold')\\n",
    "\\n",
    "metrics = ['VED', 'VES', 'EF', 'stroke_volume', 'max_pressure', 'min_pressure', 'cardiac_output', 'heart_rate']\\n",
    "\\n",
    "for i, metric in enumerate(metrics):\\n",
    "    ax = axes[i//4, i%4]\\n",
    "    datasets['metrics'][metric].hist(bins=30, ax=ax, alpha=0.7, color='lightcoral', edgecolor='black')\\n",
    "    ax.set_title(f'{metric}', fontweight='bold')\\n",
    "    ax.set_xlabel(metric)\\n",
    "    ax.set_ylabel('Frequency')\\n",
    "    ax.grid(True, alpha=0.3)\\n",
    "\\n",
    "plt.tight_layout()\\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter correlations\\n",
    "plt.figure(figsize=(12, 10))\\n",
    "correlation_matrix = datasets['parameters'].corr()\\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\\n",
    "\\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\\n",
    "            square=True, linewidths=0.5, cbar_kws={\\\"shrink\\\": .8})\\n",
    "plt.title('Windkessel Parameter Correlations', fontsize=14, fontweight='bold')\\n",
    "plt.tight_layout()\\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü©∫ 4. Clinical Condition Analysis\\n",
    "\\n",
    "Let's analyze how parameters differ across clinical conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot key parameters by condition\\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\\n",
    "fig.suptitle('Key Parameters by Clinical Condition', fontsize=16, fontweight='bold')\\n",
    "\\n",
    "key_params = ['Emax', 'Emin', 'Rs', 'Ra', 'Ca', 'Vd']\\n",
    "\\n",
    "for i, param in enumerate(key_params):\\n",
    "    ax = axes[i//3, i%3]\\n",
    "    sns.boxplot(data=datasets['conditions'], x='condition', y=param, ax=ax)\\n",
    "    ax.set_title(f'{param} by Condition', fontweight='bold')\\n",
    "    ax.set_xlabel('Clinical Condition')\\n",
    "    ax.set_ylabel(param)\\n",
    "    ax.tick_params(axis='x', rotation=45)\\n",
    "    ax.grid(True, alpha=0.3)\\n",
    "\\n",
    "plt.tight_layout()\\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display condition statistics\\n",
    "print('üìä PARAMETER STATISTICS BY CONDITION')\\n",
    "print('=' * 50)\\n",
    "\\n",
    "for condition in datasets['conditions']['condition'].unique():\\n",
    "    condition_data = datasets['conditions'][datasets['conditions']['condition'] == condition]\\n",
    "    print(f'\\\\nüè• {condition.upper()}')\\n",
    "    print('-' * 30)\\n",
    "    \\n",
    "    # Key parameters for this condition\\n",
    "    key_stats = condition_data[['Emax', 'Emin', 'Rs', 'Ra']].describe().loc[['mean', 'std']]\\n",
    "    print(key_stats.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó 5. Parameter-Metric Relationships\\n",
    "\\n",
    "Let's explore the relationships between Windkessel parameters and clinical metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot key parameter-metric relationships\\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\\n",
    "fig.suptitle('Parameter-Metric Relationships', fontsize=16, fontweight='bold')\\n",
    "\\n",
    "# Emax vs EF\\n",
    "axes[0,0].scatter(datasets['parameters']['Emax'], datasets['metrics']['EF'], alpha=0.6, color='blue')\\n",
    "axes[0,0].set_xlabel('Emax (Maximum Elastance)')\\n",
    "axes[0,0].set_ylabel('Ejection Fraction (%)')\\n",
    "axes[0,0].set_title('Contractility vs Ejection Fraction')\\n",
    "axes[0,0].grid(True, alpha=0.3)\\n",
    "\\n",
    "# Rs vs max_pressure\\n",
    "axes[0,1].scatter(datasets['parameters']['Rs'], datasets['metrics']['max_pressure'], alpha=0.6, color='red')\\n",
    "axes[0,1].set_xlabel('Rs (Systemic Resistance)')\\n",
    "axes[0,1].set_ylabel('Max Pressure (mmHg)')\\n",
    "axes[0,1].set_title('Afterload vs Peak Pressure')\\n",
    "axes[0,1].grid(True, alpha=0.3)\\n",
    "\\n",
    "# Tc vs heart_rate\\n",
    "axes[1,0].scatter(datasets['parameters']['Tc'], datasets['metrics']['heart_rate'], alpha=0.6, color='green')\\n",
    "axes[1,0].set_xlabel('Tc (Cardiac Cycle Time)')\\n",
    "axes[1,0].set_ylabel('Heart Rate (bpm)')\\n",
    "axes[1,0].set_title('Cycle Time vs Heart Rate')\\n",
    "axes[1,0].grid(True, alpha=0.3)\\n",
    "\\n",
    "# Vd vs stroke_volume\\n",
    "axes[1,1].scatter(datasets['parameters']['Vd'], datasets['metrics']['stroke_volume'], alpha=0.6, color='purple')\\n",
    "axes[1,1].set_xlabel('Vd (Dead Volume)')\\n",
    "axes[1,1].set_ylabel('Stroke Volume (mL)')\\n",
    "axes[1,1].set_title('Dead Volume vs Stroke Volume')\\n",
    "axes[1,1].grid(True, alpha=0.3)\\n",
    "\\n",
    "plt.tight_layout()\\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between parameters and metrics\\n",
    "combined_corr = datasets['combined'].corr()\\n",
    "\\n",
    "# Extract parameter-metric correlations\\n",
    "param_cols = datasets['parameters'].columns\\n",
    "metric_cols = datasets['metrics'].columns\\n",
    "\\n",
    "param_metric_corr = combined_corr.loc[param_cols, metric_cols]\\n",
    "\\n",
    "plt.figure(figsize=(12, 8))\\n",
    "sns.heatmap(param_metric_corr, annot=True, cmap='RdBu_r', center=0,\\n",
    "            square=False, linewidths=0.5, cbar_kws={\\\"shrink\\\": .8})\\n",
    "plt.title('Parameter-Metric Correlations', fontsize=14, fontweight='bold')\\n",
    "plt.xlabel('Clinical Metrics')\\n",
    "plt.ylabel('Windkessel Parameters')\\n",
    "plt.tight_layout()\\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 6. Machine Learning Data Preparation\\n",
    "\\n",
    "Let's prepare the data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\\n",
    "from sklearn.preprocessing import StandardScaler\\n",
    "from sklearn.metrics import mean_squared_error, r2_score\\n",
    "\\n",
    "# Prepare features and targets\\n",
    "X = datasets['combined'].iloc[:, :12].values  # Parameters\\n",
    "y = datasets['combined'].iloc[:, 12:].values  # Clinical metrics\\n",
    "\\n",
    "feature_names = list(datasets['parameters'].columns)\\n",
    "target_names = list(datasets['metrics'].columns)\\n",
    "\\n",
    "print(f'üìä Features (X): {X.shape} - {feature_names}')\\n",
    "print(f'üìä Targets (y): {y.shape} - {target_names}')\\n",
    "\\n",
    "# Split the data\\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\n",
    "    X, y, test_size=0.2, random_state=42\\n",
    ")\\n",
    "\\n",
    "print(f'\\\\n‚úÖ Training set: {X_train.shape}')\\n",
    "print(f'‚úÖ Test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\\n",
    "scaler_X = StandardScaler()\\n",
    "scaler_y = StandardScaler()\\n",
    "\\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\\n",
    "X_test_scaled = scaler_X.transform(X_test)\\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\\n",
    "y_test_scaled = scaler_y.transform(y_test)\\n",
    "\\n",
    "print('üìä DATA NORMALIZATION SUMMARY')\\n",
    "print('=' * 40)\\n",
    "print(f'X_train - Mean: {X_train_scaled.mean():.3f}, Std: {X_train_scaled.std():.3f}')\\n",
    "print(f'y_train - Mean: {y_train_scaled.mean():.3f}, Std: {y_train_scaled.std():.3f}')\\n",
    "print(f'X_test - Mean: {X_test_scaled.mean():.3f}, Std: {X_test_scaled.std():.3f}')\\n",
    "print(f'y_test - Mean: {y_test_scaled.mean():.3f}, Std: {y_test_scaled.std():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple baseline model demonstration\\n",
    "from sklearn.linear_model import LinearRegression\\n",
    "from sklearn.ensemble import RandomForestRegressor\\n",
    "\\n",
    "# Train simple models\\n",
    "models = {\\n",
    "    'Linear Regression': LinearRegression(),\\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\\n",
    "}\\n",
    "\\n",
    "results = {}\\n",
    "\\n",
    "for name, model in models.items():\\n",
    "    # Train model\\n",
    "    model.fit(X_train_scaled, y_train_scaled)\\n",
    "    \\n",
    "    # Predict\\n",
    "    y_pred_scaled = model.predict(X_test_scaled)\\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\\n",
    "    \\n",
    "    # Calculate metrics\\n",
    "    mse = mean_squared_error(y_test, y_pred)\\n",
    "    r2 = r2_score(y_test, y_pred)\\n",
    "    \\n",
    "    results[name] = {'MSE': mse, 'R2': r2}\\n",
    "    \\n",
    "    print(f'ü§ñ {name}:')\\n",
    "    print(f'   MSE: {mse:.3f}')\\n",
    "    print(f'   R¬≤:  {r2:.3f}')\\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 7. Echo Metadata Analysis\\n",
    "\\n",
    "Let's explore the echocardiogram metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze echo metadata\\n",
    "echo_df = datasets['echo_meta']\\n",
    "\\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\\n",
    "fig.suptitle('Echocardiogram Metadata Analysis', fontsize=16, fontweight='bold')\\n",
    "\\n",
    "# Age distribution\\n",
    "axes[0,0].hist(echo_df['age'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\\n",
    "axes[0,0].set_title('Age Distribution')\\n",
    "axes[0,0].set_xlabel('Age (years)')\\n",
    "axes[0,0].set_ylabel('Frequency')\\n",
    "axes[0,0].grid(True, alpha=0.3)\\n",
    "\\n",
    "# Gender distribution\\n",
    "gender_counts = echo_df['gender'].value_counts()\\n",
    "axes[0,1].pie(gender_counts.values, labels=gender_counts.index, autopct='%1.1f%%')\\n",
    "axes[0,1].set_title('Gender Distribution')\\n",
    "\\n",
    "# Frame count distribution\\n",
    "frame_counts = echo_df['frame_count'].value_counts().sort_index()\\n",
    "axes[0,2].bar(frame_counts.index, frame_counts.values, alpha=0.7, color='lightcoral')\\n",
    "axes[0,2].set_title('Frame Count Distribution')\\n",
    "axes[0,2].set_xlabel('Number of Frames')\\n",
    "axes[0,2].set_ylabel('Frequency')\\n",
    "axes[0,2].grid(True, alpha=0.3)\\n",
    "\\n",
    "# SNR distribution\\n",
    "axes[1,0].hist(echo_df['snr_db'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\\n",
    "axes[1,0].set_title('Signal-to-Noise Ratio')\\n",
    "axes[1,0].set_xlabel('SNR (dB)')\\n",
    "axes[1,0].set_ylabel('Frequency')\\n",
    "axes[1,0].grid(True, alpha=0.3)\\n",
    "\\n",
    "# Contrast distribution\\n",
    "axes[1,1].hist(echo_df['contrast'], bins=20, alpha=0.7, color='orange', edgecolor='black')\\n",
    "axes[1,1].set_title('Image Contrast')\\n",
    "axes[1,1].set_xlabel('Contrast')\\n",
    "axes[1,1].set_ylabel('Frequency')\\n",
    "axes[1,1].grid(True, alpha=0.3)\\n",
    "\\n",
    "# Condition distribution\\n",
    "condition_counts = echo_df['condition'].value_counts()\\n",
    "axes[1,2].pie(condition_counts.values, labels=condition_counts.index, autopct='%1.1f%%')\\n",
    "axes[1,2].set_title('Condition Distribution')\\n",
    "\\n",
    "plt.tight_layout()\\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 8. Data Export and Saving\\n",
    "\\n",
    "Let's demonstrate how to save processed data for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\\n",
    "import pickle\\n",
    "\\n",
    "# Create processed data dictionary\\n",
    "processed_data = {\\n",
    "    'X_train': X_train_scaled,\\n",
    "    'X_test': X_test_scaled,\\n",
    "    'y_train': y_train_scaled,\\n",
    "    'y_test': y_test_scaled,\\n",
    "    'scaler_X': scaler_X,\\n",
    "    'scaler_y': scaler_y,\\n",
    "    'feature_names': feature_names,\\n",
    "    'target_names': target_names\\n",
    "}\\n",
    "\\n",
    "# Save to pickle file\\n",
    "with open('../data/processed_data.pkl', 'wb') as f:\\n",
    "    pickle.dump(processed_data, f)\\n",
    "\\n",
    "print('‚úÖ Processed data saved to ../data/processed_data.pkl')\\n",
    "\\n",
    "# Save training data as CSV\\n",
    "train_df = pd.DataFrame(X_train_scaled, columns=feature_names)\\n",
    "train_targets = pd.DataFrame(y_train_scaled, columns=target_names)\\n",
    "train_combined = pd.concat([train_df, train_targets], axis=1)\\n",
    "train_combined.to_csv('../data/training_data_normalized.csv', index=False)\\n",
    "\\n",
    "print('‚úÖ Training data saved to ../data/training_data_normalized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 9. Data Summary and Statistics\\n",
    "\\n",
    "Let's create a comprehensive summary of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive data summary\\n",
    "summary_stats = {}\\n",
    "\\n",
    "for name, df in datasets.items():\\n",
    "    if name != 'echo_meta':  # Skip echo metadata for numeric summary\\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\\n",
    "        summary_stats[name] = {\\n",
    "            'shape': df.shape,\\n",
    "            'numeric_columns': len(numeric_cols),\\n",
    "            'missing_values': df.isnull().sum().sum(),\\n",
    "            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024\\n",
    "        }\\n",
    "\\n",
    "# Display summary\\n",
    "print('üìä DATASET SUMMARY STATISTICS')\\n",
    "print('=' * 50)\\n",
    "\\n",
    "for name, stats in summary_stats.items():\\n",
    "    print(f'\\\\nüìÅ {name.upper()}:')\\n",
    "    print(f'   Shape: {stats[\\\"shape\\\"]}')\\n",
    "    print(f'   Numeric columns: {stats[\\\"numeric_columns\\\"]}')\\n",
    "    print(f'   Missing values: {stats[\\\"missing_values\\\"]}')\\n",
    "    print(f'   Memory usage: {stats[\\\"memory_usage_mb\\\"]:.2f} MB')\\n",
    "\\n",
    "# Overall statistics\\n",
    "total_samples = sum(stats['shape'][0] for stats in summary_stats.values())\\n",
    "total_memory = sum(stats['memory_usage_mb'] for stats in summary_stats.values())\\n",
    "\\n",
    "print(f'\\\\nüéØ OVERALL STATISTICS:')\\n",
    "print(f'   Total samples: {total_samples:,}')\\n",
    "print(f'   Total memory: {total_memory:.2f} MB')\\n",
    "print(f'   Datasets: {len(datasets)}')\\n",
    "print(f'   Data quality: ‚úÖ No missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 10. Next Steps and Recommendations\\n",
    "\\n",
    "Based on this data exploration, here are the recommended next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üéØ NEXT STEPS AND RECOMMENDATIONS')\\n",
    "print('=' * 50)\\n",
    "print()\\n",
    "print('1. ü§ñ MACHINE LEARNING MODELS:')\\n",
    "print('   - Use the normalized training data for neural networks')\\n",
    "print('   - Try physics-informed neural networks (PINNs)')\\n",
    "print('   - Implement multi-task learning for all clinical metrics')\\n",
    "print()\\n",
    "print('2. üî¨ PHYSICS-INFORMED LEARNING:')\\n",
    "print('   - Incorporate Windkessel equations as loss constraints')\\n",
    "print('   - Use parameter-metric correlations for regularization')\\n",
    "print('   - Validate predictions against known physiological ranges')\\n",
    "print()\\n",
    "print('3. üè• CLINICAL VALIDATION:')\\n",
    "print('   - Test models on clinical condition datasets')\\n",
    "print('   - Evaluate performance across different disease states')\\n",
    "print('   - Implement uncertainty quantification')\\n",
    "print()\\n",
    "print('4. üìä DATA AUGMENTATION:')\\n",
    "print('   - Generate more samples for rare conditions')\\n",
    "print('   - Add noise for robustness testing')\\n",
    "print('   - Create time-series data for dynamic modeling')\\n",
    "print()\\n",
    "print('5. üé• VIDEO PROCESSING:')\\n",
    "print('   - Generate actual synthetic echocardiogram videos')\\n",
    "print('   - Implement 3D CNN architectures')\\n",
    "print('   - Add temporal attention mechanisms')\\n",
    "print()\\n",
    "print('‚úÖ Data is ready for advanced cardiac digital twin development!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Conclusion\\n",
    "\\n",
    "In this notebook, we have:\\n",
    "\\n",
    "‚úÖ **Loaded and explored** all synthetic cardiac datasets\\n",
    "‚úÖ **Visualized** parameter distributions and relationships\\n",
    "‚úÖ **Analyzed** clinical conditions and their characteristics\\n",
    "‚úÖ **Prepared** data for machine learning models\\n",
    "‚úÖ **Demonstrated** baseline model training\\n",
    "‚úÖ **Explored** echocardiogram metadata\\n",
    "‚úÖ **Saved** processed data for future use\\n",
    "\\n",
    "The datasets are now ready for advanced cardiac digital twin development using physics-informed neural networks and deep learning approaches.\\n",
    "\\n",
    "**ü´Ä Ready to build the future of cardiac care through AI-powered digital twins!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

